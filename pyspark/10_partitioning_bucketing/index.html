<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Partitioning & Bucketing - PySpark Learning Hub</title>
    <link rel="stylesheet" href="../../assets/css/main.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fira+Code&display=swap" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/controls/OrbitControls.js"></script>
</head>
<body>
    <header class="header">
        <div class="container header-content">
            <a href="../../index.html" class="logo">
                <svg class="logo-icon" viewBox="0 0 40 40" fill="none">
                    <rect width="40" height="40" rx="8" fill="#e25a1c"/>
                    <path d="M12 20L20 12L28 20L20 28L12 20Z" fill="white"/>
                    <circle cx="20" cy="20" r="4" fill="#e25a1c"/>
                </svg>
                <span>Data Engineering Hub</span>
            </a>
            <nav class="nav">
                <a href="../../index.html#pyspark" class="nav-link active">PySpark</a>
                <a href="../../index.html#pandas" class="nav-link">Pandas</a>
                <a href="../../index.html#sql" class="nav-link">SQL</a>
                <a href="../../index.html#etl" class="nav-link">ETL</a>
                <a href="../../index.html#datawarehouse" class="nav-link">Data Warehouse</a>
                <button class="theme-toggle">üåô</button>
            </nav>
        </div>
    </header>

    <main class="container">
        <nav class="breadcrumb">
            <span class="breadcrumb-item"><a href="../../index.html">Home</a></span>
            <span class="breadcrumb-separator">/</span>
            <span class="breadcrumb-item"><a href="../../index.html#pyspark">PySpark</a></span>
            <span class="breadcrumb-separator">/</span>
            <span class="breadcrumb-item">Partitioning & Bucketing</span>
        </nav>

        <section class="section">
            <h1>Partitioning & Bucketing</h1>
            <p>Partitioning and bucketing are data organization techniques that improve query performance by reducing the amount of data scanned. Partitioning divides data by column values, while bucketing distributes data into fixed-size buckets using hash functions.</p>

            <div class="info-box info">
                <strong>Key Concept:</strong> Partitioning is ideal for columns with low cardinality used in filters (e.g., date, region). Bucketing is better for high-cardinality columns used in joins.
            </div>

            <h2>3D Visualization: Data Organization</h2>
            <div id="visualization" class="visualization-container"></div>
            <div class="visualization-controls">
                <button class="viz-btn" onclick="showPartitioning()">Partitioning</button>
                <button class="viz-btn" onclick="showBucketing()">Bucketing</button>
                <button class="viz-btn" onclick="showCombined()">Combined</button>
            </div>

            <h2>Code Examples</h2>

            <div class="tabs">
                <button class="tab active" data-tab="partition">Partitioning</button>
                <button class="tab" data-tab="bucket">Bucketing</button>
                <button class="tab" data-tab="repartition">Repartitioning</button>
            </div>

            <div class="tab-contents">
                <div id="partition" class="tab-content active">
                    <div class="code-container">
                        <div class="code-header">
                            <span class="code-language">Python</span>
                            <button class="copy-btn">Copy</button>
                        </div>
                        <div class="code-block">
<pre>from pyspark.sql import SparkSession
from pyspark.sql.functions import col, year, month

spark = SparkSession.builder.appName("Partitioning").getOrCreate()

# Sample sales data
data = [
    ("2023-01-15", "Electronics", "USA", 1000),
    ("2023-01-20", "Clothing", "UK", 500),
    ("2023-02-10", "Electronics", "USA", 1500),
    ("2023-02-15", "Clothing", "Canada", 800),
    ("2023-03-01", "Electronics", "UK", 2000),
    ("2023-03-20", "Clothing", "USA", 600)
]
df = spark.createDataFrame(data, ["date", "category", "country", "amount"])

# Write with single partition column
df.write.mode("overwrite") \
    .partitionBy("country") \
    .parquet("/tmp/sales_by_country")

# Write with multiple partition columns
df.write.mode("overwrite") \
    .partitionBy("country", "category") \
    .parquet("/tmp/sales_by_country_category")

# Read partitioned data - partition pruning
print("Reading with partition filter (only USA data scanned):")
spark.read.parquet("/tmp/sales_by_country") \
    .filter(col("country") == "USA") \
    .explain()

# Dynamic partition overwrite
spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")
df.write.mode("overwrite") \
    .partitionBy("country") \
    .parquet("/tmp/sales_dynamic")

# Check partition structure
import os
print("\nPartition structure:")
for root, dirs, files in os.walk("/tmp/sales_by_country"):
    level = root.replace("/tmp/sales_by_country", "").count(os.sep)
    indent = " " * 2 * level
    print(f"{indent}{os.path.basename(root)}/")

# Partition by derived columns
df_with_date = df.withColumn("year", year(col("date"))) \
                 .withColumn("month", month(col("date")))

df_with_date.write.mode("overwrite") \
    .partitionBy("year", "month") \
    .parquet("/tmp/sales_by_date")

spark.stop()</pre>
                        </div>
                    </div>
                </div>

                <div id="bucket" class="tab-content">
                    <div class="code-container">
                        <div class="code-header">
                            <span class="code-language">Python</span>
                            <button class="copy-btn">Copy</button>
                        </div>
                        <div class="code-block">
<pre>from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Bucketing") \
    .config("spark.sql.warehouse.dir", "/tmp/spark-warehouse") \
    .enableHiveSupport() \
    .getOrCreate()

# Sample data
orders = spark.createDataFrame([
    (1, 1001, "2023-01-15", 100.0),
    (2, 1002, "2023-01-16", 200.0),
    (3, 1001, "2023-01-17", 150.0),
    (4, 1003, "2023-01-18", 300.0),
    (5, 1002, "2023-01-19", 250.0)
], ["order_id", "customer_id", "order_date", "amount"])

customers = spark.createDataFrame([
    (1001, "Alice", "NYC"),
    (1002, "Bob", "LA"),
    (1003, "Charlie", "Chicago")
], ["customer_id", "name", "city"])

# Write bucketed table
orders.write.mode("overwrite") \
    .bucketBy(4, "customer_id") \
    .sortBy("order_date") \
    .saveAsTable("bucketed_orders")

customers.write.mode("overwrite") \
    .bucketBy(4, "customer_id") \
    .saveAsTable("bucketed_customers")

# Join bucketed tables - no shuffle needed!
print("Bucketed Join (no shuffle):")
spark.sql("""
    SELECT o.order_id, c.name, o.amount
    FROM bucketed_orders o
    JOIN bucketed_customers c ON o.customer_id = c.customer_id
""").explain()

# Compare with non-bucketed join
orders.createOrReplaceTempView("orders")
customers.createOrReplaceTempView("customers")

print("\nNon-bucketed Join (shuffle required):")
spark.sql("""
    SELECT o.order_id, c.name, o.amount
    FROM orders o
    JOIN customers c ON o.customer_id = c.customer_id
""").explain()

# Bucketing with partitioning
orders.write.mode("overwrite") \
    .partitionBy("order_date") \
    .bucketBy(4, "customer_id") \
    .saveAsTable("partitioned_bucketed_orders")

spark.stop()</pre>
                        </div>
                    </div>
                </div>

                <div id="repartition" class="tab-content">
                    <div class="code-container">
                        <div class="code-header">
                            <span class="code-language">Python</span>
                            <button class="copy-btn">Copy</button>
                        </div>
                        <div class="code-block">
<pre>from pyspark.sql import SparkSession
from pyspark.sql.functions import col, spark_partition_id

spark = SparkSession.builder.appName("Repartitioning").getOrCreate()

# Create sample data
df = spark.range(1000000).withColumn("category", col("id") % 10)

print(f"Initial partitions: {df.rdd.getNumPartitions()}")

# repartition() - Full shuffle, creates specified number of partitions
df_repartitioned = df.repartition(20)
print(f"After repartition(20): {df_repartitioned.rdd.getNumPartitions()}")

# repartition by column - Data with same key goes to same partition
df_by_category = df.repartition(10, "category")
print(f"After repartition by category: {df_by_category.rdd.getNumPartitions()}")

# Check partition distribution
df_by_category.groupBy(spark_partition_id().alias("partition")) \
    .count() \
    .orderBy("partition") \
    .show()

# coalesce() - Reduce partitions without full shuffle
df_coalesced = df.coalesce(5)
print(f"After coalesce(5): {df_coalesced.rdd.getNumPartitions()}")

# repartitionByRange() - Range partitioning for sorted output
df_range = df.repartitionByRange(10, "id")
print(f"After repartitionByRange: {df_range.rdd.getNumPartitions()}")

# Best practices for partition size
# Target: 128MB - 1GB per partition
# Formula: num_partitions = total_data_size / target_partition_size

# Check partition sizes
def get_partition_sizes(df):
    return df.rdd.mapPartitions(
        lambda x: [sum(1 for _ in x)]
    ).collect()

sizes = get_partition_sizes(df_by_category)
print(f"Partition sizes: {sizes}")

# Adaptive Query Execution (AQE) - Spark 3.0+
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.minPartitionSize", "64MB")

spark.stop()</pre>
                        </div>
                    </div>
                </div>
            </div>

            <div style="display: flex; justify-content: space-between; margin-top: 3rem; padding-top: 2rem; border-top: 1px solid var(--border-color);">
                <a href="../09_spark_sql/index.html" style="color: var(--text-muted);">‚Üê Previous: Spark SQL</a>
                <a href="../11_caching_persistence/index.html" style="color: var(--accent-primary);">Next: Caching & Persistence ‚Üí</a>
            </div>
        </section>
    </main>

    <footer class="footer">
        <div class="container footer-content">
            <p style="margin: 0; color: var(--text-muted);">PySpark & Data Engineering Learning Hub</p>
        </div>
    </footer>

    <script src="../../assets/js/main.js"></script>
    <script src="../../assets/js/visualization.js"></script>
    <script>
        let viz;

        document.addEventListener('DOMContentLoaded', function() {
            viz = new DataVisualization('visualization', {
                backgroundColor: document.documentElement.getAttribute('data-theme') === 'dark' ? 0x1a1a2e : 0xf8f9fa,
                cameraPosition: { x: 6, y: 4, z: 6 }
            });
            showPartitioning();
        });

        function showPartitioning() {
            viz.clear();
            
            // Partitions by region
            const partitions = [
                { x: -2, label: 'USA', color: 0xe25a1c },
                { x: 0, label: 'UK', color: 0x4dabf7 },
                { x: 2, label: 'Canada', color: 0x198754 }
            ];

            partitions.forEach(p => {
                for (let i = 0; i < 3; i++) {
                    viz.createDataNode({
                        type: 'cube', size: 0.4, color: p.color,
                        position: { x: p.x, y: 1 - i * 0.6, z: 0 }
                    });
                }
                viz.createLabel(p.label, { x: p.x, y: 2, z: 0 });
            });

            viz.createLabel('Partitioning by Country', { x: 0, y: -1.5, z: 0 });
            viz.createGrid(10, 10);
        }

        function showBucketing() {
            viz.clear();
            
            // Buckets
            for (let b = 0; b < 4; b++) {
                viz.createDataNode({
                    type: 'cylinder', size: 0.6, color: 0x4dabf7,
                    position: { x: -2 + b * 1.3, y: 0, z: 0 }
                });
                viz.createLabel(`Bucket ${b}`, { x: -2 + b * 1.3, y: 1, z: 0 });
                
                // Data in buckets
                for (let i = 0; i < 2; i++) {
                    viz.createDataNode({
                        type: 'cube', size: 0.25, color: 0xe25a1c,
                        position: { x: -2 + b * 1.3 + (i - 0.5) * 0.3, y: 0.5, z: 0 }
                    });
                }
            }

            viz.createLabel('Bucketing - Hash distribution', { x: 0, y: -1.5, z: 0 });
            viz.createGrid(10, 10);
        }

        function showCombined() {
            viz.clear();
            
            // Partitions (rows) with buckets (columns)
            const partitions = ['2023-01', '2023-02'];
            const buckets = 3;

            partitions.forEach((p, pi) => {
                viz.createLabel(p, { x: -3, y: 1 - pi * 1.5, z: 0 });
                for (let b = 0; b < buckets; b++) {
                    viz.createDataNode({
                        type: 'cube', size: 0.5, color: pi === 0 ? 0xe25a1c : 0x4dabf7,
                        position: { x: -1 + b * 1.5, y: 1 - pi * 1.5, z: 0 }
                    });
                }
            });

            viz.createLabel('B0', { x: -1, y: 2.2, z: 0 });
            viz.createLabel('B1', { x: 0.5, y: 2.2, z: 0 });
            viz.createLabel('B2', { x: 2, y: 2.2, z: 0 });

            viz.createLabel('Partitioning + Bucketing Combined', { x: 0, y: -1.5, z: 0 });
            viz.createGrid(10, 10);
        }
    </script>
</body>
</html>
