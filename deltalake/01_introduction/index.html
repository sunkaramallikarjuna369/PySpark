<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introduction to Delta Lake - Data Engineering Hub</title>
    <link rel="stylesheet" href="../../assets/css/main.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
</head>
<body>
    <header class="header">
        <div class="container header-content">
            <a href="../../index.html" class="logo"><span>Data Engineering Hub</span></a>
            <nav class="nav"><a href="../../index.html#deltalake" class="nav-link active">Delta Lake</a><button class="theme-toggle">&#127769;</button></nav>
        </div>
    </header>
    <main class="container">
        <section class="section">
            <h1>Introduction to Delta Lake</h1>
            <p>Delta Lake is an open-source storage layer that brings ACID transactions, scalable metadata handling, and unified streaming/batch processing to data lakes built on Apache Spark.</p>
            
            <div id="visualization" class="visualization-container"></div>

            <h2>Delta Lake Concepts</h2>
            <div class="tabs">
                <button class="tab active" data-tab="overview">Overview</button>
                <button class="tab" data-tab="create">Create Tables</button>
                <button class="tab" data-tab="read">Read Data</button>
                <button class="tab" data-tab="write">Write Data</button>
                <button class="tab" data-tab="convert">Convert to Delta</button>
            </div>
            <div class="tab-contents">
                <div id="overview" class="tab-content active">
                    <h3>What is Delta Lake?</h3>
                    <p>Delta Lake provides reliability, performance, and governance for data lakes. Key features include:</p>
                    <ul>
                        <li><strong>ACID Transactions:</strong> Serializable isolation levels ensure data integrity</li>
                        <li><strong>Scalable Metadata:</strong> Handles petabyte-scale tables with billions of partitions</li>
                        <li><strong>Time Travel:</strong> Query previous versions of data for audits and rollbacks</li>
                        <li><strong>Schema Enforcement:</strong> Prevents bad data from corrupting tables</li>
                        <li><strong>Schema Evolution:</strong> Safely add columns without breaking pipelines</li>
                        <li><strong>Unified Batch/Streaming:</strong> Same table for batch and streaming workloads</li>
                    </ul>
                    <div class="code-container">
                        <h4>Delta Lake Architecture</h4>
                        <pre><code class="language-python"># Delta Lake stores data in Parquet format with a transaction log
# 
# Table Structure:
# /delta_table/
#   ├── _delta_log/           # Transaction log (JSON files)
#   │   ├── 00000000000000000000.json
#   │   ├── 00000000000000000001.json
#   │   └── 00000000000000000010.checkpoint.parquet
#   ├── part-00000-xxx.parquet  # Data files
#   ├── part-00001-xxx.parquet
#   └── part-00002-xxx.parquet

from pyspark.sql import SparkSession

# Create Spark session with Delta Lake support
spark = SparkSession.builder \
    .appName("DeltaLakeIntro") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()

# Check Delta Lake version
print(f"Delta Lake version: {spark.conf.get('spark.databricks.delta.version', 'N/A')}")</code></pre>
                    </div>
                </div>
                
                <div id="create" class="tab-content">
                    <h3>Creating Delta Tables</h3>
                    <div class="code-container">
                        <pre><code class="language-python">from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType
from delta.tables import DeltaTable

spark = SparkSession.builder \
    .appName("CreateDeltaTables") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()

# Method 1: Create from DataFrame
data = [
    (1, "Alice", "Engineering", 75000.0),
    (2, "Bob", "Sales", 65000.0),
    (3, "Charlie", "Marketing", 70000.0)
]
schema = StructType([
    StructField("id", IntegerType(), False),
    StructField("name", StringType(), True),
    StructField("department", StringType(), True),
    StructField("salary", DoubleType(), True)
])

df = spark.createDataFrame(data, schema)
df.write.format("delta").mode("overwrite").save("/data/delta/employees")

# Method 2: Create with partitioning
df.write.format("delta") \
    .partitionBy("department") \
    .mode("overwrite") \
    .save("/data/delta/employees_partitioned")

# Method 3: Create managed table using SQL
spark.sql("""
    CREATE TABLE IF NOT EXISTS employees_managed (
        id INT,
        name STRING,
        department STRING,
        salary DOUBLE
    )
    USING DELTA
    PARTITIONED BY (department)
""")

# Method 4: Create external table
spark.sql("""
    CREATE TABLE IF NOT EXISTS employees_external
    USING DELTA
    LOCATION '/data/delta/employees'
""")

# Method 5: Create with table properties
spark.sql("""
    CREATE TABLE IF NOT EXISTS employees_with_props (
        id INT,
        name STRING,
        department STRING,
        salary DOUBLE
    )
    USING DELTA
    TBLPROPERTIES (
        'delta.autoOptimize.optimizeWrite' = 'true',
        'delta.autoOptimize.autoCompact' = 'true'
    )
""")

# Verify table creation
spark.sql("DESCRIBE EXTENDED employees_managed").show(truncate=False)</code></pre>
                    </div>
                </div>
                
                <div id="read" class="tab-content">
                    <h3>Reading Delta Tables</h3>
                    <div class="code-container">
                        <pre><code class="language-python">from pyspark.sql import SparkSession
from delta.tables import DeltaTable

spark = SparkSession.builder \
    .appName("ReadDeltaTables") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()

# Method 1: Read using path
df = spark.read.format("delta").load("/data/delta/employees")
df.show()

# Method 2: Read using table name
df = spark.table("employees_managed")
df.show()

# Method 3: Read using SQL
spark.sql("SELECT * FROM employees_managed WHERE salary > 60000").show()

# Method 4: Read specific version (Time Travel)
df_v0 = spark.read.format("delta") \
    .option("versionAsOf", 0) \
    .load("/data/delta/employees")

# Method 5: Read as of timestamp
df_timestamp = spark.read.format("delta") \
    .option("timestampAsOf", "2024-01-01 00:00:00") \
    .load("/data/delta/employees")

# Method 6: Read with predicate pushdown (partition pruning)
df_filtered = spark.read.format("delta") \
    .load("/data/delta/employees_partitioned") \
    .filter("department = 'Engineering'")

# Check execution plan for partition pruning
df_filtered.explain(True)

# Method 7: Get table metadata
delta_table = DeltaTable.forPath(spark, "/data/delta/employees")
print(f"Table history:")
delta_table.history().show(truncate=False)

print(f"Table details:")
delta_table.detail().show(truncate=False)</code></pre>
                    </div>
                </div>
                
                <div id="write" class="tab-content">
                    <h3>Writing to Delta Tables</h3>
                    <div class="code-container">
                        <pre><code class="language-python">from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, current_timestamp

spark = SparkSession.builder \
    .appName("WriteDeltaTables") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()

# Sample data
new_employees = [
    (4, "Diana", "Engineering", 80000.0),
    (5, "Eve", "Sales", 72000.0)
]
df_new = spark.createDataFrame(new_employees, ["id", "name", "department", "salary"])

# Method 1: Append data
df_new.write.format("delta") \
    .mode("append") \
    .save("/data/delta/employees")

# Method 2: Overwrite entire table
df_new.write.format("delta") \
    .mode("overwrite") \
    .save("/data/delta/employees")

# Method 3: Overwrite specific partitions (dynamic)
df_new.write.format("delta") \
    .mode("overwrite") \
    .option("replaceWhere", "department = 'Engineering'") \
    .save("/data/delta/employees_partitioned")

# Method 4: Insert using SQL
spark.sql("""
    INSERT INTO employees_managed
    VALUES (6, 'Frank', 'Marketing', 68000.0)
""")

# Method 5: Insert overwrite partition
spark.sql("""
    INSERT OVERWRITE employees_managed
    PARTITION (department = 'Sales')
    SELECT id, name, salary FROM temp_sales_data
""")

# Method 6: Write with schema merge (add new columns)
df_with_new_col = df_new.withColumn("hire_date", current_timestamp())
df_with_new_col.write.format("delta") \
    .mode("append") \
    .option("mergeSchema", "true") \
    .save("/data/delta/employees")

# Method 7: Write with optimizations
df_new.write.format("delta") \
    .mode("append") \
    .option("optimizeWrite", "true") \
    .option("autoCompact", "true") \
    .save("/data/delta/employees")

# Method 8: Batch write with idempotency (using txnAppId and txnVersion)
df_new.write.format("delta") \
    .mode("append") \
    .option("txnAppId", "my-etl-job") \
    .option("txnVersion", "12345") \
    .save("/data/delta/employees")</code></pre>
                    </div>
                </div>
                
                <div id="convert" class="tab-content">
                    <h3>Converting to Delta Format</h3>
                    <div class="code-container">
                        <pre><code class="language-python">from pyspark.sql import SparkSession
from delta.tables import DeltaTable

spark = SparkSession.builder \
    .appName("ConvertToDelta") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()

# Method 1: Convert Parquet to Delta (in-place)
# This adds the _delta_log without rewriting data files
DeltaTable.convertToDelta(spark, "parquet.`/data/parquet/employees`")

# Method 2: Convert partitioned Parquet to Delta
DeltaTable.convertToDelta(
    spark,
    "parquet.`/data/parquet/employees_partitioned`",
    "department STRING"  # Partition schema
)

# Method 3: Convert using SQL
spark.sql("""
    CONVERT TO DELTA parquet.`/data/parquet/sales`
    PARTITIONED BY (year INT, month INT)
""")

# Method 4: Convert by reading and writing (for other formats)
# Read from CSV
df_csv = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv("/data/csv/employees.csv")

# Write as Delta
df_csv.write.format("delta") \
    .mode("overwrite") \
    .save("/data/delta/employees_from_csv")

# Method 5: Convert JSON to Delta
df_json = spark.read.json("/data/json/events/")
df_json.write.format("delta") \
    .partitionBy("event_date") \
    .mode("overwrite") \
    .save("/data/delta/events")

# Verify conversion
delta_table = DeltaTable.forPath(spark, "/data/delta/employees_from_csv")
print("Conversion successful!")
delta_table.toDF().show()
delta_table.history().show()</code></pre>
                    </div>
                </div>
            </div>
            
            <div style="display: flex; justify-content: space-between; margin-top: 3rem; padding-top: 2rem; border-top: 1px solid var(--border-color);">
                <a href="../../index.html#deltalake" style="color: var(--text-muted);">&larr; Back to Delta Lake</a>
                <a href="../02_acid_transactions/index.html" style="color: var(--accent-primary);">Next: ACID Transactions &rarr;</a>
            </div>
        </section>
    </main>
    <script src="../../assets/js/main.js"></script>
    <script src="../../assets/js/visualization.js"></script>
    <script>
        let viz;
        document.addEventListener('DOMContentLoaded', function() {
            viz = new DataVisualization('visualization', { cameraPosition: { x: 8, y: 5, z: 8 } });
            showDeltaLakeArchitecture();
        });
        
        function showDeltaLakeArchitecture() {
            viz.clear();
            
            // Transaction Log
            viz.createDataNode({ type: 'cylinder', size: 0.4, color: 0x4dabf7, position: { x: -2, y: 0.5, z: 0 } });
            viz.createLabel('Transaction Log', { x: -2, y: 1.3, z: 0 });
            
            // Data Files (Parquet)
            for (let i = 0; i < 3; i++) {
                viz.createDataNode({ type: 'cube', size: 0.35, color: 0x51cf66, position: { x: 1 + i * 0.8, y: 0.3, z: -0.5 } });
            }
            viz.createLabel('Parquet Files', { x: 2, y: 1.1, z: -0.5 });
            
            // Delta Table wrapper
            viz.createDataNode({ type: 'sphere', size: 0.3, color: 0xff6b6b, position: { x: 0, y: 1.5, z: 0 } });
            viz.createLabel('Delta Table', { x: 0, y: 2.2, z: 0 });
            
            // Arrows
            viz.createArrow({ x: 0, y: 1.2, z: 0 }, { x: -1.5, y: 0.7, z: 0 }, { color: 0x888888 });
            viz.createArrow({ x: 0, y: 1.2, z: 0 }, { x: 1.5, y: 0.5, z: -0.3 }, { color: 0x888888 });
            
            viz.createGrid(8, 8);
        }
    </script>
</body>
</html>
