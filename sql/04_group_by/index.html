<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GROUP BY - SQL Learning Hub</title>
    <link rel="stylesheet" href="../../assets/css/main.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
</head>
<body>
    <header class="header">
        <div class="container header-content">
            <a href="../../index.html" class="logo"><span>Data Engineering Hub</span></a>
            <nav class="nav"><a href="../../index.html#sql" class="nav-link active">SQL</a><button class="theme-toggle">ðŸŒ™</button></nav>
        </div>
    </header>
    <main class="container">
        <section class="section">
            <h1>GROUP BY</h1>
            <p>GROUP BY groups rows with the same values into summary rows. Learn aggregations using standard SQL, PySpark DataFrame API, and PySpark SQL.</p>
            <div id="visualization" class="visualization-container"></div>
            <div class="visualization-controls">
                <button class="viz-btn" onclick="showGroupBy()">GROUP BY</button>
                <button class="viz-btn" onclick="showAggregate()">Aggregates</button>
            </div>
            <h2>Code Examples</h2>
            <div class="tabs">
                <button class="tab active" data-tab="sql">Standard SQL</button>
                <button class="tab" data-tab="pyspark-df">PySpark DataFrame</button>
                <button class="tab" data-tab="pyspark-sql">PySpark SQL</button>
                <button class="tab" data-tab="comparison">Side-by-Side</button>
            </div>
            <div class="tab-contents">
                <div id="sql" class="tab-content active">
                    <div class="code-container">
                        <div class="code-header"><span class="code-language">SQL</span><button class="copy-btn">Copy</button></div>
                        <div class="code-block">
<pre>-- Basic GROUP BY with COUNT
SELECT department_id, COUNT(*) AS employee_count
FROM employees GROUP BY department_id;

-- GROUP BY multiple columns
SELECT department_id, job_id, COUNT(*) AS count
FROM employees GROUP BY department_id, job_id;

-- SUM, AVG, MIN, MAX
SELECT department_id,
    SUM(salary) AS total_salary,
    AVG(salary) AS avg_salary,
    MIN(salary) AS min_salary,
    MAX(salary) AS max_salary
FROM employees GROUP BY department_id;

-- GROUP BY with ORDER BY
SELECT department_id, COUNT(*) AS count
FROM employees GROUP BY department_id ORDER BY count DESC;

-- ROLLUP (hierarchical subtotals)
SELECT department_id, job_id, SUM(salary)
FROM employees GROUP BY ROLLUP(department_id, job_id);

-- CUBE (all combinations)
SELECT department_id, job_id, SUM(salary)
FROM employees GROUP BY CUBE(department_id, job_id);</pre>
                        </div>
                    </div>
                </div>
                <div id="pyspark-df" class="tab-content">
                    <div class="code-container">
                        <div class="code-header"><span class="code-language">Python (PySpark DataFrame API)</span><button class="copy-btn">Copy</button></div>
                        <div class="code-block">
<pre>from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, sum, avg, min, max, countDistinct
from pyspark.sql.functions import collect_list, concat_ws

spark = SparkSession.builder.appName("GroupBy").getOrCreate()

# Create sample DataFrame
data = [
    (1, "Alice", 50000, 10, "Engineer"),
    (2, "Bob", 60000, 10, "Engineer"),
    (3, "Charlie", 75000, 20, "Manager"),
    (4, "Diana", 55000, 20, "Analyst")
]
df = spark.createDataFrame(data, ["id", "name", "salary", "dept_id", "job"])

# Basic GROUP BY with count
df.groupBy("dept_id").count().show()

# GROUP BY with multiple aggregations
df.groupBy("dept_id").agg(
    count("*").alias("employee_count"),
    sum("salary").alias("total_salary"),
    avg("salary").alias("avg_salary"),
    min("salary").alias("min_salary"),
    max("salary").alias("max_salary")
).show()

# GROUP BY multiple columns
df.groupBy("dept_id", "job").count().show()

# GROUP BY with ORDER BY
df.groupBy("dept_id").count().orderBy(col("count").desc()).show()

# Count distinct
df.groupBy("dept_id").agg(countDistinct("job").alias("unique_jobs")).show()

# Collect list (like GROUP_CONCAT)
df.groupBy("dept_id").agg(
    collect_list("name").alias("employees"),
    concat_ws(", ", collect_list("name")).alias("employee_names")
).show(truncate=False)

# ROLLUP (hierarchical subtotals)
df.rollup("dept_id", "job").agg(sum("salary").alias("total")).show()

# CUBE (all combinations)
df.cube("dept_id", "job").agg(sum("salary").alias("total")).show()

# Pivot table
df.groupBy("dept_id").pivot("job").agg(sum("salary")).show()</pre>
                        </div>
                    </div>
                </div>
                <div id="pyspark-sql" class="tab-content">
                    <div class="code-container">
                        <div class="code-header"><span class="code-language">Python (PySpark SQL)</span><button class="copy-btn">Copy</button></div>
                        <div class="code-block">
<pre>from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("GroupBy").getOrCreate()

# Create and register temp view
data = [
    (1, "Alice", 50000, 10, "Engineer"),
    (2, "Bob", 60000, 10, "Engineer"),
    (3, "Charlie", 75000, 20, "Manager"),
    (4, "Diana", 55000, 20, "Analyst")
]
df = spark.createDataFrame(data, ["id", "name", "salary", "dept_id", "job"])
df.createOrReplaceTempView("employees")

# Basic GROUP BY
spark.sql("SELECT dept_id, COUNT(*) AS count FROM employees GROUP BY dept_id").show()

# Multiple aggregations
spark.sql("""
    SELECT dept_id,
        COUNT(*) AS employee_count,
        SUM(salary) AS total_salary,
        AVG(salary) AS avg_salary,
        MIN(salary) AS min_salary,
        MAX(salary) AS max_salary
    FROM employees GROUP BY dept_id
""").show()

# GROUP BY multiple columns
spark.sql("SELECT dept_id, job, COUNT(*) FROM employees GROUP BY dept_id, job").show()

# GROUP BY with ORDER BY
spark.sql("""
    SELECT dept_id, COUNT(*) AS count 
    FROM employees GROUP BY dept_id ORDER BY count DESC
""").show()

# Count distinct
spark.sql("SELECT dept_id, COUNT(DISTINCT job) AS unique_jobs FROM employees GROUP BY dept_id").show()

# Collect list
spark.sql("SELECT dept_id, collect_list(name) AS employees FROM employees GROUP BY dept_id").show()

# ROLLUP
spark.sql("SELECT dept_id, job, SUM(salary) FROM employees GROUP BY ROLLUP(dept_id, job)").show()

# CUBE
spark.sql("SELECT dept_id, job, SUM(salary) FROM employees GROUP BY CUBE(dept_id, job)").show()

# GROUPING SETS
spark.sql("""
    SELECT dept_id, job, SUM(salary) FROM employees 
    GROUP BY GROUPING SETS ((dept_id, job), (dept_id), ())
""").show()</pre>
                        </div>
                    </div>
                </div>
                <div id="comparison" class="tab-content">
                    <div class="code-container">
                        <div class="code-header"><span class="code-language">Side-by-Side Comparison</span><button class="copy-btn">Copy</button></div>
                        <div class="code-block">
<pre># ============================================
# GROUP BY: DataFrame API vs SQL
# ============================================
from pyspark.sql import SparkSession
from pyspark.sql.functions import count, sum, avg, col

spark = SparkSession.builder.appName("Comparison").getOrCreate()

data = [(1, "Alice", 50000, 10), (2, "Bob", 60000, 10), (3, "Charlie", 75000, 20)]
df = spark.createDataFrame(data, ["id", "name", "salary", "dept_id"])
df.createOrReplaceTempView("employees")

# BASIC GROUP BY
# DataFrame API:
df.groupBy("dept_id").count().show()
# PySpark SQL:
spark.sql("SELECT dept_id, COUNT(*) FROM employees GROUP BY dept_id").show()

# MULTIPLE AGGREGATIONS
# DataFrame API:
df.groupBy("dept_id").agg(
    count("*").alias("count"),
    sum("salary").alias("total"),
    avg("salary").alias("average")
).show()
# PySpark SQL:
spark.sql("""
    SELECT dept_id, COUNT(*) AS count, SUM(salary) AS total, AVG(salary) AS average
    FROM employees GROUP BY dept_id
""").show()

# GROUP BY WITH ORDER BY
# DataFrame API:
df.groupBy("dept_id").count().orderBy(col("count").desc()).show()
# PySpark SQL:
spark.sql("SELECT dept_id, COUNT(*) AS cnt FROM employees GROUP BY dept_id ORDER BY cnt DESC").show()

# ROLLUP
# DataFrame API:
df.rollup("dept_id").agg(sum("salary")).show()
# PySpark SQL:
spark.sql("SELECT dept_id, SUM(salary) FROM employees GROUP BY ROLLUP(dept_id)").show()</pre>
                        </div>
                    </div>
                </div>
            </div>
            <div style="display: flex; justify-content: space-between; margin-top: 3rem; padding-top: 2rem; border-top: 1px solid var(--border-color);">
                <a href="../03_joins/index.html" style="color: var(--text-muted);">&larr; Previous: JOINs</a>
                <a href="../05_having/index.html" style="color: var(--accent-primary);">Next: HAVING &rarr;</a>
            </div>
        </section>
    </main>
    <script src="../../assets/js/main.js"></script>
    <script src="../../assets/js/visualization.js"></script>
    <script>
        let viz;
        document.addEventListener('DOMContentLoaded', function() {
            viz = new DataVisualization('visualization', { cameraPosition: { x: 5, y: 3, z: 5 } });
            showGroupBy();
        });
        function showGroupBy() {
            viz.clear();
            const colors = [0xe25a1c, 0x4dabf7, 0x198754];
            for (let g = 0; g < 3; g++) {
                for (let i = 0; i < 3 - g; i++) {
                    viz.createDataNode({ type: 'cube', size: 0.35, color: colors[g], position: { x: -2 + g * 2, y: 1 - i * 0.6, z: 0 } });
                }
            }
            viz.createLabel('GROUP BY - Group rows by column', { x: 0, y: -2, z: 0 });
            viz.createGrid(10, 10);
        }
        function showAggregate() {
            viz.clear();
            viz.createDataNode({ type: 'cube', size: 0.8, color: 0xe25a1c, position: { x: -1.5, y: 0.5, z: 0 } });
            viz.createDataNode({ type: 'cube', size: 0.8, color: 0x4dabf7, position: { x: 0, y: 0.5, z: 0 } });
            viz.createDataNode({ type: 'cube', size: 0.8, color: 0x198754, position: { x: 1.5, y: 0.5, z: 0 } });
            viz.createLabel('SUM', { x: -1.5, y: 1.5, z: 0 });
            viz.createLabel('AVG', { x: 0, y: 1.5, z: 0 });
            viz.createLabel('COUNT', { x: 1.5, y: 1.5, z: 0 });
            viz.createLabel('Aggregate Functions', { x: 0, y: -1.5, z: 0 });
            viz.createGrid(10, 10);
        }
    </script>
</body>
</html>
